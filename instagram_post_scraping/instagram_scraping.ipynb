{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "# TODO: Stop mechanism after all hidden comments are expanded\n",
    "# TODO: Stop mechanism after end of page is reached and nothing to load\n",
    "# TODO: Timeout mechanism for expandallcomments - done - todebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "##############################Imports:\n",
    "import argparse\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import tqdm\n",
    "import datetime\n",
    "import dateutil.parser as dparser\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################Constants:\n",
    "    #Scraper delays(s):\n",
    "\n",
    "DELAY_GETPOSTDATA = 0.1\n",
    "DELAY_GETALLPOSTDATA = 0.1\n",
    "DELAY_SCROLLER = 1\n",
    "DELAY_COMMENT_EXPANDER = 0.1\n",
    "THREAD_NUMBER = 4\n",
    "\n",
    "##############################Global vars:\n",
    "global URL_TO_SCRAPE, XLSX_OUTPUT_FILE_NAME, VERBOSE\n",
    "\n",
    "URL_TO_SCRAPE = \"https://www.instagram.com/aleksandrskrivickis\"#args['input_addr']\n",
    "XLSX_OUTPUT_FILE_NAME = (\"./instagram_dump\" + \"_\" + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\") + \".xlsx\")#, required=Falseargs['output_file']\n",
    "VERBOSE = False#args['verbose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    }
   ],
   "source": [
    "##############################Methods:\n",
    "\n",
    "def pprint(txt_pri = \"\\n\"):\n",
    "    if VERBOSE:\n",
    "        print(str(txt_pri))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def exportData(lst_pst_dat):\n",
    "    #Export to file:\n",
    "    lst_pst_dat = sum(lst_pst_dat, [])\n",
    "    df = pd.DataFrame.from_dict(lst_pst_dat)\n",
    "    df = df.astype(str)\n",
    "    df = df.reindex([\"post_id\", 'post_link', \"image_link\", \"date\", \"post\", \"post_author\", \"likes\", \"comment\", \"comment_author\"], axis=1)\n",
    "    df.index.name = \"entry\"\n",
    "    #Sort by post date, keep post and comment order\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "    df_sorted = df.sort_values(['date', 'entry'], ascending=[True, True])\n",
    "    df_sorted.reset_index(inplace=True)\n",
    "    df_sorted = df_sorted.iloc[::-1]\n",
    "    df_sorted.drop(labels=[\"index\", \"entry\"], axis=1, inplace=True)\n",
    "    df_sorted.to_excel(XLSX_OUTPUT_FILE_NAME, index=False)\n",
    "    driver.quit()\n",
    "    pprint(\"Finished!\")\n",
    "\n",
    "def getDataFromPostList_Multithread(lst_lnk):\n",
    "    threads = []\n",
    "\n",
    "    #posts = list(range(1, 1000))\n",
    "    tmp = allPosts.copy()\n",
    "    posts_ = splitListToSublists(lst_lnk, THREAD_NUMBER)\n",
    "\n",
    "    for a in range(0,THREAD_NUMBER):\n",
    "        threads.append(\"threading.Thread(target=getDataFromPostList, args=(\" + str(posts_[a]) + \",)).start()\")  \n",
    "\n",
    "    for a in threads:\n",
    "        exec(a)\n",
    "\n",
    "def getLinks():\n",
    "    #Getting all the post links\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(URL_TO_SCRAPE)\n",
    "    #time.sleep(10)\n",
    "    allPosts = scrollPageToBottomAndFindPostLinks(driver)\n",
    "    \n",
    "    #Concatenate all lists in one\n",
    "    allPosts = sum(allPosts, [])\n",
    "    #Remove dupes from posts\n",
    "    allPosts = list(set(allPosts))\n",
    "    allPosts = [\"https://www.instagram.com\" + x for x in allPosts]\n",
    "    pprint(\"Amount of links to posts scraped: \" + str(len(allPosts)))\n",
    "    driver.close()\n",
    "    return allPosts\n",
    "\n",
    "def getDataFromPostList(post_links):\n",
    "    global allPostData\n",
    "    allPostData = []\n",
    "    #Parsing every post\n",
    "    driver = webdriver.Chrome()\n",
    "    for link in tqdm.tqdm_notebook(post_links, desc=\"Parsing post data\"):\n",
    "        driver.get(link)\n",
    "        time.sleep(DELAY_GETALLPOSTDATA)\n",
    "        allPostData.append(getPostData(driver))\n",
    "    \n",
    "    driver.close()\n",
    "    return allPostData\n",
    "\n",
    "def splitListToSublists(posts = [], split_parts = 4):\n",
    "    if split_parts == 0 or split_parts == 1:\n",
    "        return posts\n",
    "    else:\n",
    "        returnable = []\n",
    "        pprint(\"Splitting \" + str(len(posts)) + \" posts\")\n",
    "        step = int(len(posts) / split_parts)\n",
    "        splitted = 0\n",
    "        for a in range(0, split_parts):\n",
    "            temp = []\n",
    "            if splitted < split_parts - 1:\n",
    "                for i in range(0, step):\n",
    "                    temp.append(posts.pop(i))\n",
    "                splitted += 1\n",
    "            else:#Last fraction of a list\n",
    "                for b in posts:\n",
    "                    temp.append(b)\n",
    "            returnable.append(temp)\n",
    "        return returnable\n",
    "\n",
    "\n",
    "def updDelayScroller():\n",
    "    global DELAY_SCROLLER\n",
    "    DELAY_SCROLLER = round(random.uniform(1, 2), 2)\n",
    "    \n",
    "def scrollRandomUp(driver):\n",
    "    for a in range(1, random.randint(1, 5)):\n",
    "        driver.execute_script(\"window.scrollBy(0,\" + str(-(random.randint(768, 1055))) + \")\")\n",
    "        time.sleep(round(random.uniform(0.1, 0.5), 2))\n",
    "    \n",
    "def scrollPageToBottomAndFindPostLinks(driver):\n",
    "    time.sleep(5)\n",
    "    #Get total amount of posts:\n",
    "    \n",
    "    try:\n",
    "        totalPosts = int(driver.find_element_by_xpath(\"//*[@id=\\\"react-root\\\"]/section/main/div/header/section/ul/li[1]/span/span\").text.replace(\",\", \"\"))\n",
    "    except Exception as Ex:\n",
    "        pprint(Ex)\n",
    "        pprint(\"Unable to locate amount of posts, using 99999 instead.\")\n",
    "        totalPosts = 9999\n",
    "\n",
    "    pbar = tqdm.tqdm_notebook(total=totalPosts, desc=\"Getting links for all the posts\")\n",
    "\n",
    "    def scrl(attempts=0, allPosts=[]):\n",
    "        pprint(\"Attempt number: \" + str(attempts) + \" / \" + str(totalPosts / 8) + \"\\n AllPosts len is: \" + str(len(allPosts)))\n",
    "        if attempts < (totalPosts / 8):\n",
    "            prevHeight = 0\n",
    "            newHeight = 1\n",
    "            while prevHeight != newHeight:\n",
    "                prevHeight = int(driver.execute_script(\"return document.body.scrollHeight;\"))\n",
    "                scrollRandomUp(driver)\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                time.sleep(DELAY_SCROLLER)\n",
    "                updDelayScroller()#Random delay\n",
    "                allPosts.append(findPostLinks(driver))\n",
    "                newHeight = int(driver.execute_script(\"return document.body.scrollHeight;\"))\n",
    "                pbar.update(16) \n",
    "            attempts += 1\n",
    "            return scrl(attempts, allPosts)\n",
    "        else:\n",
    "            return allPosts\n",
    "    \n",
    "    \n",
    "    allPosts = scrl()\n",
    "    return allPosts\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def findPostLinks(driver):\n",
    "    posts = []\n",
    "    src = driver.page_source\n",
    "    src_splitted = src.split(\"</div>\")\n",
    "\n",
    "    for entry in src_splitted:\n",
    "        postRegex = re.findall(\"(<a href=\\\")(.*)(\\?taken-by=\\w*\\\">)\", entry)\n",
    "        if len(postRegex) > 0:\n",
    "            posts.append(postRegex[0][1])\n",
    "    return posts\n",
    "\n",
    "def getPostData(driver):\n",
    "    time.sleep(DELAY_GETPOSTDATA)\n",
    "    postData = []\n",
    "    expandAllComments(driver)\n",
    "    post_link = driver.current_url\n",
    "    \n",
    "    try:\n",
    "        image_link = driver.find_element_by_xpath(\"//*[@id=\\\"react-root\\\"]/section/main/div/div/article/div[1]/div/div/div[2]\").text\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            image_link = driver.find_elements_by_tag_name('img')[1].get_attribute('src')\n",
    "        except Exception as ex:\n",
    "            image_link = \"\"\n",
    "            \n",
    "    #Getting comments\n",
    "    try:\n",
    "        authors, comments = getAllCommentsFromArticle(driver)\n",
    "    except Exception as ex:\n",
    "        authors = []\n",
    "        comments = []\n",
    "        pprint(\"Exception in getPostData() - unable to get comments from article: \" + str(post_link))\n",
    "    #Getting likes\n",
    "    try:\n",
    "        likes = driver.find_element_by_xpath(\"//*[@id=\\\"react-root\\\"]/section/main/div/div/article/div[2]/section[2]/div/span/span\").text#driver.find_element_by_tag_name('span').text.splitlines()[6].replace(\" likes\", \"\")\n",
    "    except Exception as e:\n",
    "        pprint(\"Unable to get likes from post: \" + post_link + \"\\n Trying different approach...\")\n",
    "        try:\n",
    "            likes = (len(driver.find_element_by_xpath(\"//*[@id=\\\"react-root\\\"]/section/main/div/div/article/div[2]/section[2]/div\").text.split(\",\")) + 1)\n",
    "        except Exception as Ex:\n",
    "            pprint(\"Different approach didn't work. Value of likes field will be \\\"Exception\\\"\")\n",
    "            likes = \"Exception\"\n",
    "            \n",
    "    #Getting and processing date\n",
    "    try:\n",
    "        date = driver.find_element_by_xpath(\"//*[@id=\\\"react-root\\\"]/section/main/div/div/article/div[2]/div[2]/a/time\").text#driver.find_element_by_tag_name('article').text.splitlines()[len(driver.find_element_by_tag_name('article').text.splitlines()) - 2]\n",
    "    except Exception as e:\n",
    "        pprint(\"Unable to get date from post: \" + post_link)\n",
    "        date = \"Exception\"\n",
    "        \n",
    "    if (\",\" not in date):\n",
    "        pprint(\"Date has a weird format.. \" + str(date) + \" converting...\")\n",
    "        date = date + \", \" + str(datetime.datetime.now().year)\n",
    "    date = convertDate(date)\n",
    "    pprint(date)\n",
    "    \n",
    "    if date == \"\" or \" days ago\" in driver.find_element_by_xpath(\"//*[@id=\\\"react-root\\\"]/section/main/div/div/article/div[2]/div[2]/a/time\").text.casefold():\n",
    "        try:\n",
    "            daysAgo = driver.find_element_by_xpath(\"//*[@id=\\\"react-root\\\"]/section/main/div/div/article/div[2]/div[2]/a/time\").text.casefold().replace(\" days ago\", \"\").replace(\" day ago\", \"\")\n",
    "        except Exception as ex:\n",
    "            pprint(\"Exception in getPostData()'s date conversion getting days ago from web page.\")\n",
    "            daysAgo = 0\n",
    "        try:\n",
    "            date = datetime.datetime.now() - timedelta(days=int(daysAgo))\n",
    "        except Exception as ex:\n",
    "            pprint(\"Exception in getPostData()'s date conversion(daysAgo) part.\")\n",
    "    ######################            \n",
    "    try:\n",
    "        post_id = str((driver.execute_script(\"return window._sharedData;\"))[\"entry_data\"][\"PostPage\"][0][\"graphql\"][\"shortcode_media\"][\"id\"])\n",
    "    except Exception as ex:\n",
    "        pprint(\"Exception in getPostData()'s post_id part.\")   \n",
    "        post_id = \"00000000\"\n",
    "                \n",
    "\n",
    "    firstRun = True\n",
    "    cnt = 0\n",
    "    for a, c in zip(authors, comments):\n",
    "        if firstRun:\n",
    "            firstRun = False\n",
    "            postData.append({\"post_id\": post_id, \"post_link\" : post_link, \"image_link\" : image_link, \"post_author\" : a, \"post\" : c, \"likes\": likes, \"date\": date, \"comment\": \"N/A\", \"comment_author\": \"N/A\"})\n",
    "        else:\n",
    "            postData.append({\"post_id\": str(post_id + \"_\" + str(cnt)), \"post_link\" : post_link, \"image_link\" : image_link, \"post_author\" : authors[0], \"post\" : comments[0], \"likes\": \"N/A\", \"date\": date, \"comment\": c, \"comment_author\": a})\n",
    "            cnt += 1\n",
    "    return postData\n",
    "\n",
    "def getAllCommentsFromArticle(driver):\n",
    "    #Posts description and authors name is a very first comment's content and authors name\n",
    "    authors = []\n",
    "    comments = []\n",
    "    article = driver.find_element_by_tag_name(\"article\")\n",
    "    comment = article.find_elements_by_tag_name(\"li\")\n",
    "    firstRun = True\n",
    "    for com in comment:\n",
    "        #print(\"\\n\" + com.find_element_by_tag_name(\"a\").text + \", post: \" + com.find_element_by_tag_name(\"span\").text)\n",
    "        postAuthor = driver.find_element_by_xpath(\"//*[@id=\\\"react-root\\\"]/section/main/div/div/article/header/div[2]/div[1]/div[1]/a\").text\n",
    "        if firstRun:\n",
    "            if com.find_element_by_tag_name(\"a\").text != postAuthor:\n",
    "                authors.append(\"\")\n",
    "                comments.append(\"\")\n",
    "            firstRun = False\n",
    "        authors.append(com.find_element_by_tag_name(\"a\").text)\n",
    "        comments.append(com.find_element_by_tag_name(\"span\").text)\n",
    "    return authors, comments\n",
    "\n",
    "def expandAllComments(driver):\n",
    "    oldCommentAmount = 0\n",
    "    newCommentAmount = 0\n",
    "    timeoutCounter = 10\n",
    "    try:\n",
    "        el = driver.find_element_by_xpath(\"//*[@id=\\\"react-root\\\"]/section/main/div/div/article/div[2]/div[1]/ul/li[2]/a\")\n",
    "        while (el.text == \"Load more comments\" or \"View all\" in el.text):\n",
    "            el = driver.find_element_by_xpath(\"//*[@id=\\\"react-root\\\"]/section/main/div/div/article/div[2]/div[1]/ul/li[2]/a\")\n",
    "            if (el.text == \"Load more comments\" or \"View all\" in el.text or timeoutCounter > 0):\n",
    "                oldCommentAmount = len(driver.find_element_by_tag_name(\"article\").find_elements_by_tag_name(\"li\"))\n",
    "                el.click()\n",
    "            time.sleep(DELAY_COMMENT_EXPANDER)\n",
    "            newCommentAmount = len(driver.find_element_by_tag_name(\"article\").find_elements_by_tag_name(\"li\"))\n",
    "            if (oldCommentAmount == newCommentAmount and timeoutCounter > 0):\n",
    "                timeoutCounter -= 1\n",
    "                print(\"expandAllComments() timed out. Attempts left: \" + str(timeoutCounter))\n",
    "            \n",
    "    except Exception as e:\n",
    "        pprint(\"expandAllComments() - expanded\")\n",
    "        \n",
    "def convertDate(date):\n",
    "    returnable = []\n",
    "    try:\n",
    "        if (str(date) == \"NaT\" or str(date) == \"\"):\n",
    "            currDate = str(date)\n",
    "            returnable == \"\"\n",
    "        else:\n",
    "            currDate = str(date)\n",
    "            returnable = dparser.parse(str(date)).date()\n",
    "    except Exception as e:\n",
    "        pprint(\"Exception in convertAllDates() \" + str(e))\n",
    "        pprint(\"Exception in convertAllDates() caused by \" + currDate + \" instead of date string...\")\n",
    "        returnable = \"\"\n",
    "    return returnable           \n",
    "\n",
    "def parseArgs():\n",
    "    parser = argparse.ArgumentParser(description='Instagram scraper allows to dump all the public posts and comments from a specified link to a profile.')\n",
    "    parser.add_argument('-i', '--input_addr', help='Address of an instagram profile to scrape from', required=True)\n",
    "    parser.add_argument('-o', '--output_file', help='Output file name', default=\"./instagram_dump\" + \"_\" + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\") + \".xlsx\", required=False)\n",
    "    parser.add_argument('-v', '--verbose', help='Show additional information or alerts', required=False, default=True, type=bool, choices=[True, False])\n",
    "    args = vars(parser.parse_args())\n",
    "    \n",
    "    VERBOSE = args['verbose']\n",
    "\n",
    "    pprint(\"Profile address: \" + args['input_addr'])\n",
    "    pprint(\"Output file name: \" + args['output_file'])\n",
    "\n",
    "    return args    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 10 posts\n",
      "Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a018fffe48346b7be8f02aaeaf33e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Parsing post data', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9db13a3a434e01b545f8af5517cf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Parsing post data', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53327acc247d4571bcfc42f1d67683bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Parsing post data', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71c56c661094fbba58f1b1966e4f0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Parsing post data', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################Main me\n",
    "if __name__ == \"__main__\":\n",
    "    #args = parseArgs()\n",
    "    links = getLinks()\n",
    "    #data = getDataFromPostList(links)#Single thread\n",
    "    data = getDataFromPostList_Multithread(links)\n",
    "    exportData(allPostData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
